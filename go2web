#!/usr/bin/env python3
import socket
import argparse
from bs4 import BeautifulSoup
from urllib.parse import urlparse, parse_qs
import ssl
import os
import time

CACHE_DIR = "cache"
EXPIRATION_TIME = 10 # seconds

def fetch_url(url, redirect_count=0):
  # Check if URL is provided
  if not url:
    return
  
  # Check if URL is cached
  cached_response = load_from_cache(url)
  if cached_response:
    print("(Cache): Fetching from cache...")
    print(cached_response)
    return
  
  # Check if URL is valid
  try:
    result = urlparse(url)
    print(f"URL: {url}")
    if result.scheme and result.netloc:
      url_tmp = result.geturl()
      print(f"URL is valid: {url_tmp}")
    elif result.path and not result.scheme:
      url_tmp = 'https://' + url
      result = urlparse(url_tmp)
      if not all([result.scheme, result.netloc]):
        print(f"URL is invalid: {url}")
        return
    else:
      print(f"URL is invalid: {url}")
      return
  except ValueError:
    print(f"URL is invalid: {url}")
    return

  if result.scheme == 'https':
    port = 443
    context = ssl.create_default_context()
    sock = context.wrap_socket(socket.socket(socket.AF_INET), server_hostname=result.netloc)
  else:
    port = 80
    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)

  recv_size = 4096
  
  try:
    sock.connect((result.netloc, port))
  except socket.gaierror:
    print(f"Unable to connect: {url} is not a known hostname")
    return
  
  request = f"GET {result.path or '/'} HTTP/1.1\r\nHost: {result.netloc}\r\nConnection: close\r\n\r\n"
  sock.send(request.encode())

  response = b""
  while True:
    part = sock.recv(recv_size)
    if part:
      response += part
    else:
      break

    # Parse the headers from the response
  headers, body = response.split(b'\r\n\r\n', 1)
  headers = headers.decode().split('\r\n')

  # Find the Content-Type header
  content_type = None
  for header in headers:
    if header.lower().startswith('content-type:'):
      content_type = header.split(':', 1)[1].strip()

  # Check the content type
  if content_type == 'application/json':
    print('This URL returns JSON data')
  elif content_type == 'text/html':
    print('This URL returns HTML data')
  else:
    print('This URL returns data of type: ' + content_type)

  sock.close()

  response = response.decode(errors='ignore')
  status_code = int(response.split(' ')[1])
  redirection_status_codes = [301, 302, 303, 307, 308]

  print(f"Status code: {status_code}")
  if status_code in redirection_status_codes:
    if redirect_count >= 5:
      print("(Status): Too many redirects. Exiting...")
      return

    location = next(line for line in response.split('\r\n') if line.startswith('Location: '))
    new_url = location.split(': ')[1]
    fetch_url(new_url, redirect_count = redirect_count + 1)
  else:
    soup = BeautifulSoup(response, "html.parser")
    text = soup.get_text()
    print(text) 

    save_to_cache(url, text)

    tags = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'a']
    tag_texts = {tag: [] for tag in tags}

    for tag in tags:
      elements = soup.find_all(tag)
      for element in elements:
        tag_texts[tag].append(element.get_text())

    for tag, texts in tag_texts.items():
      print(f"{tag}: {texts}")   

def save_to_cache(url, response):
  if not os.path.exists(CACHE_DIR):
    os.makedirs(CACHE_DIR)

  filename = url_to_filename(url)
  with open(os.path.join(CACHE_DIR, filename), 'w') as file:
    file.write(response)

  with open(os.path.join(CACHE_DIR, filename + "_time"), 'w') as file:
    file.write(str(time.time()))    

def load_from_cache(url):
  filename = os.path.join(CACHE_DIR, url_to_filename(url))

  if os.path.exists(filename) and os.path.exists(filename + "_time"):
    with open(filename + "_time", 'r') as file:
      timestamp = float(file.read())

    if time.time() - timestamp < EXPIRATION_TIME:
      with open(filename, 'r') as file:
        return file.read()

  return None

def url_to_filename(url):
  replace_chars = {
      '/': '_',
      ':': '_',
      '?': '_',
      '.': '_',
      '+': '_',
      ' ': '_'
  }

  for char, replacement in replace_chars.items():
    url = url.replace(char, replacement)

  return url  

def fetch_search(search):
  if search is None:
    return

  cached_response = load_from_cache(search)
  if cached_response:
    print("(Cache): Fetching from cache...")
    print(cached_response)
    return
  
  search = search.replace(' ', '+')
  sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
  sock.connect(("www.google.com", 80))
  request = f"GET /search?q={search} HTTP/1.1\r\nHost: www.google.com\r\nConnection: close\r\n\r\n"
  sock.send(request.encode())

  response = b""
  while True:
      part = sock.recv(4096)
      if part:
          response += part
      else:
          break

  sock.close()
  soup = BeautifulSoup(response, "html.parser")
  links_title = soup.find_all('h3')
  links_elements = [links_title[i].find_parent('a') for i in range(len(links_title))]

  # print("titles:", links_title)
  # print("\nelements:", links_elements)

  result = ''
  for i, link in enumerate(links_elements):
    if link is None:
      continue

    url = link.get('href')
    parsed_url = urlparse(url)
    actual_url = parse_qs(parsed_url.query)['q']

    print("Link", i+1, "=", actual_url[0])
    result += f"Link {i+1}: {actual_url[0]}\n"
  
  save_to_cache(search, result)

def parse_args():
  parser = argparse.ArgumentParser(description="HTTP Client", add_help=False)
  parser.add_argument("-u", "--url", help="make an HTTP request to the specified URL and print the response")
  parser.add_argument("-s", "--search", nargs="+", help="make an HTTP request to search the term(s) using Google search engine and print top 10 results")
  parser.add_argument("-h", "--help", action="help", default=argparse.SUPPRESS,
                      help="show all the options and their description")
  args = parser.parse_args()

  if args.search:
    args.search = ' '.join(args.search)

  if not any(vars(args).values()):
    parser.print_help()
    exit()

  return args

if __name__ == "__main__":
  args = parse_args()
  fetch_url(args.url)
  fetch_search(args.search)